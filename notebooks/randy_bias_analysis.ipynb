{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa93cf8",
   "metadata": {},
   "source": [
    "# May Code Pudding: Bias Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebae15a",
   "metadata": {},
   "source": [
    "## Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5b1fc8",
   "metadata": {},
   "source": [
    "This project explores the detection of textual bias in news articles using a combination of expert-labeled datasets and natural language processing techniques. The primary dataset used is SG2 from the BABE (Bias Annotations By Experts) corpus, which contains over 3,000 sentences labeled by domain experts as either biased or non-biased. Supplementary data includes a bias word lexicon and headline datasets labeled as biased or neutral.\n",
    "\n",
    "The goal of this project is to develop a machine learning model that can accurately predict whether a given sentence or headline exhibits bias. To achieve this, the project combines TF-IDF vectorization of text with handcrafted features such as the number of matched bias-related words. The resulting model is trained using logistic regression and evaluated with AUC-ROC and accuracy metrics.\n",
    "\n",
    "This notebook also includes detailed exploratory data analysis (EDA), a custom pipeline for preprocessing, and a test phase on new unseen headlines to assess model generalization. Visualization techniques are applied to better understand the distribution of bias and the relationship between lexical signals and human-labeled bias.\n",
    "\n",
    "The project is designed to serve as both a research exploration into bias detection and a reproducible framework for evaluating bias in text-based datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da17b1ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ✅ How This Project Addressed Key Requirements\n",
    "\n",
    "**1. Load a labeled dataset and explore its contents**  \n",
    "✅ Done in **Cell Blocks 5, 10, 15, 17, 21, 23, 26, 30, 34**  \n",
    "- Loaded SG2, the bias word lexicon, and headline datasets.\n",
    "- Removed `No agreement` rows and mapped labels.\n",
    "- Explored dataset size, source distribution, and label balance.\n",
    "- Added `lexicon_match_count` and merged sentence context with headlines.\n",
    "\n",
    "**2. Create a pipeline with a model that predicts bias in text**  \n",
    "✅ Done in **Cell Blocks 39, 43, 50, 52, 56, 59, 63**  \n",
    "- Built a pipeline using `TfidfVectorizer`, `StandardScaler`, and `LogisticRegression`.\n",
    "- Used `train_test_split` to divide data and fit the model.\n",
    "- Applied predictions using helper functions to evaluate new sentences and full articles.\n",
    "\n",
    "**3. Extract interpretable features from the text**  \n",
    "✅ Done in **Cell Blocks 6, 15, 35, 38, 39, 102, 132**  \n",
    "- Created `lexicon_match_count` using a bias word lexicon.\n",
    "- Constructed `combined_text` from sentence and headline pairs.\n",
    "- Tagged missing headlines with `[NO_TITLE]` to preserve meaning.\n",
    "- Reused these engineered features in downstream Wikipedia analysis.\n",
    "\n",
    "**4. Evaluate the model with AUC and accuracy**  \n",
    "✅ Done in **Cell Blocks 43, 50, 56, 93, 160, 163**  \n",
    "- Assessed model with `roc_auc_score`, `accuracy_score`, and `classification_report`.\n",
    "- Used predicted probabilities and thresholds to interpret model output.\n",
    "- Analyzed most biased predictions and compared model sensitivity to bias.\n",
    "\n",
    "**Bonus: Visualize Bias Across Samples**  \n",
    "✅ Done in **Cell Blocks 23, 61, 66, 72, 74, 82, 88, 93**  \n",
    "- Plotted distributions of word counts, bias scores, and lexicon matches.\n",
    "- Highlighted sentence-level bias scoring and outlier analysis.\n",
    "- Visualized article-wide bias trends and prediction confidence ranges.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d7507d",
   "metadata": {},
   "source": [
    "## Getting Packages and Reading Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd0ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             roc_auc_score)\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a1f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.pardir, \"datasets\")\n",
    "\n",
    "df_sg2 = pd.read_csv(os.path.join(dataset_path, \"final_labels_SG2.csv\"), sep=\";\")\n",
    "df_lex = pd.read_excel(os.path.join(dataset_path, \"bias_word_lexicon.xlsx\"))\n",
    "\n",
    "df_bias = pd.read_csv(os.path.join(dataset_path, \"news_headlines_usa_biased.csv\"))\n",
    "df_neutral = pd.read_csv(os.path.join(dataset_path, \"news_headlines_usa_neutral.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4597be4",
   "metadata": {},
   "source": [
    "────────────────────────────────────────────────────────────────────────────T\n",
    "\n",
    "This code loads a collection of datasets that will be used to train and evaluate a model for detecting bias in text.\n",
    "\n",
    "**First**, it sets up the file path to the folder where all the data is stored. The path points one level above the current folder into a directory called `datasets`. This helps keep the project organized and ensures anyone running the code can find the files, no matter where the script is located.\n",
    "\n",
    "**Next**, it loads three labeled datasets named `SG1`, `SG2`, and `MBIC`. These files contain sentences that have already been marked as either biased or non-biased. They use semicolons instead of commas to separate the data, which is why the `sep=';'` setting is used. These datasets are important because they give the model real examples of what biased and non-biased text looks like.\n",
    "\n",
    "**Then**, it reads in a file called `bias_word_lexicon.xlsx`, which is an Excel file containing a list of words commonly linked to bias. This list can be used to measure how many potentially biased words appear in a sentence.\n",
    "\n",
    "**Finally**, it loads two more datasets: one containing biased news headlines and one containing neutral ones. These shorter texts can help the model recognize how bias appears even in small snippets of text.\n",
    "\n",
    "Altogether, this step is about preparing all the raw data the model will need — including examples, labels, and word lists — so that the rest of the project can run smoothly.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "**Randy's Update**\n",
    "\n",
    "I loaded only the `SG2` dataset and excluded the others from the analysis because they contain duplicate sentences. \n",
    "\n",
    "Here is a blurb about the BABE source that might be worth mentioning:\n",
    "\n",
    "We decided to use the BABE (Bias Annotations By Experts) datasets for this project, which provides sentence- and word-level textual bias annotation for news articles. \n",
    "\n",
    "BABE includes three datasets with different methodologies:\n",
    "- MBIC contains 1,700 sentences labeled by crowdsourcers\n",
    "- SG1 contains the same 1,700 sentences labeled by 8 expert annotators\n",
    "- SG2 contains the same 1,700 expert-annotated sentences plus 2,000 more sentences labeled by 5 expert annotators\n",
    "\n",
    "Source:\n",
    "- https://www.kaggle.com/datasets/timospinde/babe-media-bias-annotations-by-experts\n",
    "- https://aclanthology.org/2021.findings-emnlp.101.pdf\n",
    "\n",
    "We chose the SG2 dataset for most of the project because it contains the most data, was annotated by experts, and has roughly balanced biased and unbiased classes. We also supplemented the data with applicable news headlines and a bias word lexicon provided by BABE. Later in this notebook, we compared the MBIC and SG1 datasets to gain more insight into how bias ratings differ between crowdsourced and expert bias ratings. \n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a6f378",
   "metadata": {},
   "source": [
    "## Getting Data Merged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee37673",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99125a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg2[\"label_bias\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a6f3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg2 = df_sg2[df_sg2[\"label_bias\"] != \"No agreement\"]\n",
    "df_sg2[\"label\"] = df_sg2[\"label_bias\"].map({\"Biased\": 1, \"Non-biased\": 0})\n",
    "df_sg2.drop(columns=\"label_bias\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c2c3d9",
   "metadata": {},
   "source": [
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "This result shows how many examples of each label type exist in the `label_bias` column of the `df_sg1` dataset.\n",
    "\n",
    "**`Non-biased` – 800 rows**  \n",
    "These are sentences that were labeled as clearly *not biased*. They are likely written in a neutral or factual tone.\n",
    "\n",
    "**`Biased` – 746 rows**  \n",
    "These are sentences that were labeled as *biased*. They probably contain emotionally charged language or show a one-sided opinion.\n",
    "\n",
    "**`No agreement` – 154 rows**  \n",
    "These are sentences where the people labeling the data *could not agree* on whether the sentence was biased or not. This means the sentence was unclear, confusing, or too balanced to confidently label.\n",
    "\n",
    "The value counts tell us that the dataset is fairly balanced between biased and non-biased examples, but there's a smaller group of uncertain cases that may need to be removed or handled differently when training a machine learning model.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "**Randy's Update**\n",
    "\n",
    "`Non-biased` – 1863 rows\n",
    "\n",
    "`Biased` – 1810 rows\n",
    "\n",
    "`No agreement` – 1 row\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b22eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg2[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_words_set = set(df_lex.iloc[:, 0].str.lower().dropna())\n",
    "\n",
    "df_sg2[\"lexicon_match_count\"] = df_sg2[\"text\"].apply(\n",
    "    lambda x: sum(word in bias_words_set for word in str(x).lower().split())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a7475",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bias[\"label\"] = 1\n",
    "df_neutral[\"label\"] = 0\n",
    "headline_full = pd.concat([df_bias, df_neutral], ignore_index=True)\n",
    "\n",
    "df_sg2 = df_sg2.merge(\n",
    "    headline_full[[\"url\", \"title\"]], left_on=\"news_link\", right_on=\"url\", how=\"left\"\n",
    ")\n",
    "\n",
    "df_sg2[\"combined_text\"] = df_sg2.apply(\n",
    "    lambda row: (\n",
    "        f\"{row['title']}. {row['text']}\" if pd.notnull(row[\"title\"]) else row[\"text\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "df_sg2.drop(columns=[\"url\", \"title\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885def4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg2[\"combined_text\"] = df_sg2.apply(\n",
    "    lambda row: (\n",
    "        row[\"combined_text\"]\n",
    "        if row[\"combined_text\"] != row[\"text\"]\n",
    "        else f\"[NO_TITLE] {row['text']}\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec3b0a",
   "metadata": {},
   "source": [
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "This code prepares the final dataset used to train a model that detects bias in text.\n",
    "\n",
    "First, it **removes rows** from all three labeled datasets where the label was `\"No agreement\"`, since these examples are unclear.\n",
    "\n",
    "It then **creates a new column called `label`** where:\n",
    "- `\"Biased\"` becomes `1`\n",
    "- `\"Non-biased\"` becomes `0`\n",
    "\n",
    "Next, it counts how many biased words appear in each sentence using the list stored in the `'biased_words'` column, and stores that number in a new column called `bias_word_count`.\n",
    "\n",
    "For the `df_mbic` dataset, only the most important columns are kept: the sentence, label, biased word count, and the list of biased words.\n",
    "\n",
    "All three datasets are then **combined into one**, called `combined_df`.\n",
    "\n",
    "Then, it checks each sentence and counts how many words match the ones in the `bias_word_lexicon.xlsx` file, storing that number in a new column called `lexicon_match_count`.\n",
    "\n",
    "Next, it loads two headline datasets and assigns a label (`1` for biased, `0` for neutral), then combines them.\n",
    "\n",
    "The code tries to **attach each sentence to a headline**, if one exists. It builds a new column called `combined_text` that includes the headline and the sentence. If there's no headline, it just uses the sentence but adds a tag like `[NO_TITLE]` to let the model know one wasn’t found.\n",
    "\n",
    "The final result is a rich dataset where each row has:\n",
    "- A cleaned sentence\n",
    "- A label\n",
    "- Extra features like how many biased words it contains\n",
    "- An optional headline for added context\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "**Randy's Update**\n",
    "\n",
    "Since we are only using SG2 now, I eliminated `combined_df` and replaced it with `df_sg2` throughout the notebook. \n",
    "\n",
    "We are no longer using `bias_word_count`, so I deleted it from the code. \n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296810a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a26761-3edd-4051-bd9a-9b5660519e27",
   "metadata": {},
   "source": [
    "# Exploring SG2\n",
    "\n",
    "SG2 (accounting for additions made over the course of data preprocessing) contains `text`, a source `news_link`, the `outlet` the text is from, the `topic`, a `type` (left, center, or right), a `label_opinion` rating whether the headline is factual or opinionated or a mixture, a `label` where factual headlines are 0 and opinionated ones are 1, a list of `biased_words`, and a `lexicon_match_count` that shows how many of the words in the headline show up in `bias_word_lexicon.xlsx`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792469e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sg2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f6b5f0-d436-4aeb-aab5-3fc4e3f9b44c",
   "metadata": {},
   "source": [
    "### How long are the texts in SG2? ###\n",
    "\n",
    "We can get word count from `pandas.Series.str.len` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89adafc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sg2[\"text\"].str.lower().str.split().str.len().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0332863-513e-4a33-ad83-8254d9e52d55",
   "metadata": {},
   "source": [
    "The longest text in the corpus contains 99 words, while the shortest contains just one word: 'may'. (It's index 1706; the next shortest one was 'Cable news is poison', which is coherent enough to be judged for bias.) The median word count is 31, and the standard deviation is 11.66.\n",
    "\n",
    "To visualize this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e34152",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg2[\"text\"].str.lower().str.split().str.len().plot(\n",
    "    title=\"Word counts in SG2\",\n",
    "    kind=\"hist\",\n",
    "    bins=30,\n",
    "    figsize=[8, 5],\n",
    "    xlabel=\"Words in text\",\n",
    "    ylabel=\"Number of entries\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c249e4-2fdc-4be0-8492-4c0d89af479e",
   "metadata": {},
   "source": [
    "We can see something loosely resembling a normal distribution here, where word counts under 20 and above 40 were both relatively rare.\n",
    "\n",
    "### Which outlets were represented?\n",
    "\n",
    "Simple enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71d8cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg2[\"outlet\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg2[\"outlet\"].value_counts().plot(\n",
    "    title=\"Outlets represented by SG2\",\n",
    "    kind=\"bar\",\n",
    "    figsize=[10, 4],\n",
    "    xlabel=\"Outlet\",\n",
    "    rot=30,\n",
    "    ylabel=\"Entries\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2318aa3d-8db4-4df1-9344-33651bfc7d09",
   "metadata": {},
   "source": [
    "We can see that Breitbart, Alternet, and Reuters were by far the most common sources in the data set. The New York Times was cited exactly once, making it the least common.\n",
    "\n",
    "### Among left- and right-biased texts, which outlets were the most common?\n",
    "\n",
    "To do that, we can do similarly to the above, but take only the entries with the corresponding `type`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ac9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg2[df_sg2[\"type\"] == \"left\"][\"outlet\"].value_counts().plot(\n",
    "    title=\"Outlets represented by SG2, left-leaning entries only\",\n",
    "    kind=\"bar\",\n",
    "    figsize=[9, 4],\n",
    "    xlabel=\"Outlet\",\n",
    "    rot=30,\n",
    "    ylabel=\"Entries\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f8b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg2[df_sg2[\"type\"] == \"center\"][\"outlet\"].value_counts().plot(\n",
    "    title=\"Outlets represented by SG2, center-leaning entries only\",\n",
    "    kind=\"bar\",\n",
    "    figsize=[9, 4],\n",
    "    xlabel=\"Outlet\",\n",
    "    rot=30,\n",
    "    ylabel=\"Entries\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184138dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg2[df_sg2[\"type\"] == \"right\"][\"outlet\"].value_counts().plot(\n",
    "    title=\"Outlets represented by SG2, right-leaning entries only\",\n",
    "    kind=\"bar\",\n",
    "    figsize=[9, 4],\n",
    "    xlabel=\"Outlet\",\n",
    "    rot=30,\n",
    "    ylabel=\"Entries\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd2f8cc-1244-43c7-b74c-46bdc48fe19b",
   "metadata": {},
   "source": [
    "We can see here that the `type` is basically categorizing the `outlet`: SG2 considers Breitbart, Federalist, and Fox News to be right-leaning sources; Alternet, HuffPost, and MSNBC to be left-leaning sources; and Reuters and USA Today to be centrist ones. Daily Beast, Daily Stormer, and New York Times were not categorized in this manner.\n",
    "\n",
    "### Which media outlets in the data most frequently presented biased text?\n",
    "\n",
    "An important note: this question is fairly controversial in nature. We can only describe what's going on in our data, so take the results with the understanding that they reflect that scope!\n",
    "\n",
    "To answer this, we need to group the data by outlet, then chart the number of `label==1`. Since our outlets aren't equally represented, we can use the mean rather than the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bfaa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg2.groupby(\"outlet\")[\"label\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2348fc4a-fdb0-4df6-a121-c06f95250502",
   "metadata": {},
   "source": [
    "For the chart, we'll drop the three outlets that are barely represented in the data (Daily Beast, Daily Stormer, and New York Times) since they have small sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f20d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_sg2.groupby(\"outlet\")[\"label\"]\n",
    "    .mean()\n",
    "    .drop([\"Daily Beast\", \"Daily Stormer\", \"New York Times\"], axis=0)\n",
    "    .sort_values(ascending=False)\n",
    "    * 100\n",
    ").plot(\n",
    "    title=\"Percentage of biased sentences in SG2 by source\",\n",
    "    kind=\"bar\",\n",
    "    figsize=[9, 4],\n",
    "    xlabel=\"Source\",\n",
    "    rot=30,\n",
    "    ylabel=\"Percent\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e93779-e8fe-489e-bd6e-42b013ae6553",
   "metadata": {},
   "source": [
    "We see a gradual decline in bias percentage between any two outlets. Notably, the left- and right-identified outlets more or less alternate with each other, suggesting SG1 went out of its way to include an even mix of such sources. Unsurprisingly, the center-identified outlets, USA Today and Reuters, are also seen as relatively unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa89e9e6-523c-481b-84c5-fe44d360b5c6",
   "metadata": {},
   "source": [
    "### How many words match the bias word lexicon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c371dc84-7046-46bf-baf9-8d036f09994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg2[\"lexicon_match_count\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72207e39-2180-4e72-8283-d4533a145810",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sg2[\"lexicon_match_count\"].plot(\n",
    "    title=\"Lexicon Match Count in SG2\",\n",
    "    kind=\"hist\",\n",
    "    bins=30,\n",
    "    figsize=[8, 5],\n",
    "    xlabel=\"Lexicon Match Count\",\n",
    "    ylabel=\"Number of Sentences\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e6485-41db-433d-96b1-8eaccf4c71cc",
   "metadata": {},
   "source": [
    "The vast majority of sentences in the SG2 dataset (79%) have no matching words from the bias word lexicon included with the BABE source. A moderate number of sentences have 1 or 2 matching words (21%), and a negligible number (21 sentences) have between 3 and 5 matching words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37b6ac",
   "metadata": {},
   "source": [
    "## Getting Merged Data Ready for Modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c973f8f-8dac-4164-a386-ae8f6fc69eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54e0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_sg2[\"combined_text\"]\n",
    "y = df_sg2[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b9b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_feature = \"combined_text\"\n",
    "numeric_features = [\"lexicon_match_count\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000, ngram_range=(1, 2), stop_words=\"english\"\n",
    ")\n",
    "classifier = LogisticRegression(max_iter=1000, penalty=\"l2\", C=1.0, solver=\"liblinear\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text\", vectorizer, text_feature),\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[(\"preprocessing\", preprocessor), (\"classifier\", classifier)])\n",
    "\n",
    "X = df_sg2[[text_feature] + numeric_features]\n",
    "y = df_sg2[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f3e9e0",
   "metadata": {},
   "source": [
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "This is the initial evaluation of the model's performance using a test set of 2,031 examples.\n",
    "\n",
    "The model is trying to predict whether a sentence is **biased (`1`)** or **non-biased (`0`)**.\n",
    "\n",
    "**For label 0 (non-biased):**\n",
    "- Precision: 0.87 → When the model predicts non-biased, it is correct 87% of the time.\n",
    "- Recall: 0.89 → It correctly finds 89% of the actual non-biased sentences.\n",
    "- F1-score: 0.88 → A balanced measure of both precision and recall.\n",
    "\n",
    "**For label 1 (biased):**\n",
    "- Precision: 0.90 → When the model predicts bias, it is correct 90% of the time.\n",
    "- Recall: 0.88 → It correctly detects 88% of the truly biased sentences.\n",
    "- F1-score: 0.89 → Again, a strong balance.\n",
    "\n",
    "**Overall accuracy**: 89% of all predictions were correct.\n",
    "\n",
    "**ROC AUC: 0.9508**  \n",
    "This is a measure of how well the model separates the two classes. A perfect model scores 1.0, and random guessing is 0.5.  \n",
    "A score of **0.95** means the model is *very good* at telling biased and non-biased sentences apart.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "**Randy's Update**\n",
    "\n",
    "I removed `bias_word_count` feature and all four categorical features (`outlet`, `topic`, `type`, `label_opinion`) because they will not be available for the scraped Wikipedia articles. \n",
    "\n",
    "Using only `SG2` but retaining all the features made almost no difference in model performance:\n",
    "\n",
    "- **Overall accuracy**: 89% \n",
    "\n",
    "- **ROC AUC**: 0.9495  \n",
    "\n",
    "Using only `SG2` and removing the features listed above severely decreased model performance:\n",
    "\n",
    "- **Overall accuracy**: 73% \n",
    "\n",
    "- **ROC AUC**: 0.8039\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f87798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "classifier = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text\", vectorizer, text_feature),\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[(\"preprocessing\", preprocessor), (\"classifier\", classifier)])\n",
    "\n",
    "param_grid = {\n",
    "    \"preprocessing__text__max_features\": [5000, 10000, 15000, 20000],\n",
    "    \"preprocessing__text__ngram_range\": [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5)],\n",
    "    \"classifier__C\": [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "}\n",
    "\n",
    "X = df_sg2[[text_feature] + numeric_features]\n",
    "y = df_sg2[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring=\"roc_auc\", cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best AUC:\", grid_search.best_score_)\n",
    "print(\"Best Params:\", grid_search.best_params_)\n",
    "\n",
    "y_proba = grid_search.predict_proba(X_test)[:, 1]\n",
    "print(\"Final Test ROC AUC:\", roc_auc_score(y_test, y_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b4fe6",
   "metadata": {},
   "source": [
    "## Data Scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed92d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "wiki = wikipediaapi.Wikipedia(\n",
    "    language=\"en\", user_agent=\"BiasDetectionProject/1.0 (betaknight@yourdomain.com)\"\n",
    ")\n",
    "\n",
    "\n",
    "def fetch_article(title):\n",
    "    page = wiki.page(title)\n",
    "    if page.exists():\n",
    "        return page.text\n",
    "    else:\n",
    "        raise ValueError(f\"Article '{title}' not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251ab8a4",
   "metadata": {},
   "source": [
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "This code lets us pull text from Wikipedia.\n",
    "\n",
    "It sets up a connection to Wikipedia using English and a custom user agent.  \n",
    "The `fetch_article` function takes an article title, grabs the page, and returns its full text if it exists. If not, it shows an error message.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb81804-5d5a-478d-8204-866e0fd44aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_test_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "accuracies = [accuracy_score(y_test, y_test_proba > t) for t in thresholds]\n",
    "\n",
    "best_threshold = thresholds[np.argmax(accuracies)]\n",
    "print(f\"Best threshold for Accuracy: {best_threshold:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(thresholds, accuracies, label=\"Accuracy\")\n",
    "plt.axvline(\n",
    "    best_threshold,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Best Threshold = {best_threshold:.2f}\",\n",
    ")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy by Classification Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14300117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def predict_bias_from_article(title, model):\n",
    "    article_text = fetch_article(title)\n",
    "    sentences = sent_tokenize(normalize_text(article_text))\n",
    "\n",
    "    temp_df = pd.DataFrame({\"combined_text\": sentences})\n",
    "    temp_df[\"lexicon_match_count\"] = temp_df[\"combined_text\"].apply(\n",
    "        lambda x: sum(word in bias_words_set for word in str(x).lower().split())\n",
    "    )\n",
    "\n",
    "    preds = model.predict(temp_df)\n",
    "    proba = model.predict_proba(temp_df)[:, 1]\n",
    "    preds = (proba > best_threshold).astype(int)\n",
    "    bias_score = proba.mean()\n",
    "\n",
    "    return {\n",
    "        \"bias_score\": round(bias_score, 3),\n",
    "        \"biased_sentences\": int(preds.sum()),\n",
    "        \"total_sentences\": len(sentences),\n",
    "        \"sentences\": sentences,\n",
    "        \"predictions\": preds,\n",
    "        \"probabilities\": proba,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0dae7c",
   "metadata": {},
   "source": [
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "This function checks how biased a Wikipedia article is.\n",
    "\n",
    "It fetches the article, splits it into sentences, and creates a DataFrame.  \n",
    "Some default values are added to match what the model was trained on.  \n",
    "It then uses the model to predict how biased each sentence is, using a custom threshold of `0.33`.  \n",
    "It returns the bias score (percent of biased sentences), the sentence predictions, and their probabilities.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "**Randy's Update**\n",
    "\n",
    "I ran an analysis to optimize the threshold for accuracy and found the best value to be `0.47`, so that is what the function uses.\n",
    "\n",
    "I added a `normalize_text` function to trim white space.\n",
    "\n",
    "I removed the hardcoded features (`bias_word_count`, `outlet`, `topic`, `type`, and `label_opinion`) that contribute nothing to the model and computed the `lexicon_match_count` for each article.\n",
    "\n",
    "I changed the bias score from percent of biased sentences to the average probability of bias across sentences. It smoothed out the scores and reshuffled the order of topics a bit, but it did not substantively change the results. If we prefer the intuitive appeal of saying the bias score is the percent of biased sentences, we can change it back. \n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0aa5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuilding based on recent best results\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=20000, ngram_range=(1, 4), stop_words=\"english\"\n",
    ")\n",
    "classifier = LogisticRegression(max_iter=1000, solver=\"liblinear\", C=10.0)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text\", vectorizer, text_feature),\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[(\"preprocessing\", preprocessor), (\"classifier\", classifier)])\n",
    "\n",
    "X = df_sg2[[text_feature] + numeric_features]\n",
    "y = df_sg2[\"label\"]\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee7bcca",
   "metadata": {},
   "source": [
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "This code rebuilds and trains the final machine learning pipeline.\n",
    "\n",
    "It uses `TfidfVectorizer` to turn text into numbers, looking at up to 3-word phrases and limiting to 20,000 features.  \n",
    "It one-hot encodes categories like outlet and topic, and standardizes numeric columns.  \n",
    "All features are combined and passed into a logistic regression model.  \n",
    "The full pipeline is then trained using the labeled dataset.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc48efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict_bias_from_article(\"Donald Trump\", pipeline)\n",
    "\n",
    "print(\n",
    "    f\"Bias Score: {results['bias_score']} ({results['biased_sentences']} of {results['total_sentences']} sentences)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b05e5ec",
   "metadata": {},
   "source": [
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "This runs the bias prediction function on the Wikipedia article for \"Donald Trump\".\n",
    "\n",
    "It prints a final score showing that **63 out of 557 sentences** were predicted as biased,  \n",
    "resulting in a **bias score of 0.113**, or **11.3%** of the article.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "**Randy's Update**\n",
    "\n",
    "The bias score is now much higher for the \"Donald Trump\" article:\n",
    "\n",
    "- **252 out of 563 sentences** were predicted as biased\n",
    "- **bias score = 0.445**\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b5bc91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sent, prob in sorted(\n",
    "    zip(results[\"sentences\"], results[\"probabilities\"]),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True,\n",
    "):\n",
    "    if prob > best_threshold:\n",
    "        print(f\"⚠️ {round(prob, 3)}: {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efc02f9",
   "metadata": {},
   "source": [
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "These are the sentences from the \"Donald Trump\" Wikipedia article that the model flagged as biased, using a threshold of **0.33**.\n",
    "\n",
    "Each line shows the **bias probability score** followed by the sentence.  \n",
    "Most sentences are between **0.33 and 0.45**, which means they aren't obviously biased, but may contain **framing, emotionally charged words, or subtle implications** the model picked up on.\n",
    "\n",
    "Examples include:\n",
    "- Highlighting **wealth and privilege** in early life\n",
    "- Mentioning **bankruptcies** and **legal troubles**\n",
    "- Using phrases like `\"racist or misogynistic\"` or `\"promoted conspiracy theories\"`\n",
    "\n",
    "This confirms the model can detect **subtle linguistic bias**, even in an article that follows an encyclopedic style.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "**Randy's Update**\n",
    "\n",
    "I changed the threshold to `0.47`, which flagged many more sentences with biase scores ranging between **0.47 and 0.98**, but the general analysis probably still applies. \n",
    "\n",
    "I also sorted the sentences by bias to make them easier to evaluate. \n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf9bca",
   "metadata": {},
   "source": [
    "## Creating Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59ad7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wikipedia_articles(\n",
    "    titles, model, output_file=\"../scraped_data/wiki_bias_predictions.csv\"\n",
    "):\n",
    "    all_data = []\n",
    "\n",
    "    for title in titles:\n",
    "        try:\n",
    "            text = fetch_article(title)\n",
    "            sentences = sent_tokenize(normalize_text(text))\n",
    "\n",
    "            temp_df = pd.DataFrame({\"combined_text\": sentences})\n",
    "            temp_df[\"lexicon_match_count\"] = temp_df[\"combined_text\"].apply(\n",
    "                lambda x: sum(word in bias_words_set for word in str(x).lower().split())\n",
    "            )\n",
    "\n",
    "            preds = model.predict(temp_df)\n",
    "            proba = model.predict_proba(temp_df)[:, 1]\n",
    "\n",
    "            temp_df[\"bias_prediction\"] = preds\n",
    "            temp_df[\"bias_probability\"] = proba\n",
    "            temp_df[\"article_title\"] = title\n",
    "            temp_df[\"sentence_index\"] = temp_df.index\n",
    "\n",
    "            all_data.append(temp_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error: {title} — {e}\")\n",
    "\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d249a951",
   "metadata": {},
   "source": [
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "This function takes in a list of Wikipedia article titles and runs **bias prediction** on every sentence from every article.\n",
    "\n",
    "For each title:\n",
    "- It fetches the article and splits it into sentences.\n",
    "- A temporary dataset is created with default metadata (like outlet and topic) to match what the model expects.\n",
    "- The model predicts bias for each sentence and adds the result and the bias probability.\n",
    "- It adds the article name and sentence index for organization.\n",
    "\n",
    "All the sentence results from all articles are combined into one dataset and saved as a single CSV file called `wiki_bias_predictions.csv`.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "**Randy's Update**\n",
    "\n",
    "I normalized the text, removed the hardcoded features (`bias_word_count`, `outlet`, `topic`, `type`, and `label_opinion`) that contribute nothing to the model, and computed the `lexicon_match_count` for each article.\n",
    "\n",
    "I returned the `final_df` to do some EDA. \n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e394caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    \"Donald Trump\",\n",
    "    \"Joe Biden\",\n",
    "    \"Kamala Harris\",\n",
    "    \"Barack Obama\",\n",
    "    \"Ron DeSantis\",\n",
    "    \"Bernie Sanders\",\n",
    "    \"Tea Party movement\",\n",
    "    \"QAnon\",\n",
    "    \"Pro-life\",\n",
    "    \"Pro-choice\",\n",
    "    \"Abortion in the United States\",\n",
    "    \"Gun control\",\n",
    "    \"Second Amendment\",\n",
    "    \"Immigration to the United States\",\n",
    "    \"Border wall\",\n",
    "    \"Transgender rights\",\n",
    "    \"LGBT adoption\",\n",
    "    \"Same-sex marriage\",\n",
    "    \"Gender identity\",\n",
    "    \"Critical race theory\",\n",
    "    \"Affirmative action\",\n",
    "    \"Fox News\",\n",
    "    \"MSNBC\",\n",
    "    \"CNN\",\n",
    "    \"Breitbart News\",\n",
    "    \"The New York Times\",\n",
    "    \"Israeli-Palestinian conflict\",\n",
    "    \"Hamas\",\n",
    "    \"Ukraine war\",\n",
    "    \"Russian invasion of Ukraine\",\n",
    "    \"NATO\",\n",
    "    \"Taliban\",\n",
    "    \"Evangelicalism\",\n",
    "    \"Islamophobia\",\n",
    "    \"Christian nationalism\",\n",
    "    \"Religious freedom in the United States\",\n",
    "    \"Climate change\",\n",
    "    \"COVID-19 pandemic\",\n",
    "    \"Vaccine hesitancy\",\n",
    "    \"Misinformation\",\n",
    "    \"Flat Earth\",\n",
    "    \"Creationism\",\n",
    "    \"Police brutality\",\n",
    "    \"Black Lives Matter\",\n",
    "    \"Stop and frisk\",\n",
    "    \"War on drugs\",\n",
    "    \"Norway\",\n",
    "    \"Baseball\",\n",
    "    \"Apple pie\",\n",
    "    \"Moon\",\n",
    "    \"Nintendo\",\n",
    "    \"Water cycle\",\n",
    "    \"Mount Everest\",\n",
    "    \"Planet Earth\",\n",
    "    \"Pencil\",\n",
    "    \"Library\",\n",
    "    \"Train station\",\n",
    "    \"Paper\",\n",
    "    \"Clock\",\n",
    "    \"Miocene\",\n",
    "    \"Milan\",\n",
    "    \"Pangea\",\n",
    "    \"Jogging\",\n",
    "    \"Crustacean\",\n",
    "    \"Sand dune\",\n",
    "    \"Origami\",\n",
    "    \"Bicycle\",\n",
    "    \"Caterpillar\",\n",
    "    \"Seahorse\",\n",
    "    \"Quartz\",\n",
    "    \"Umbrella\",\n",
    "    \"Refrigerator\",\n",
    "    \"Metronome\",\n",
    "    \"Snowman\",\n",
    "    \"Fire extinguisher\",\n",
    "    \"Symbiosis\",\n",
    "    \"Euclid\",\n",
    "    \"Photosynthesis\",\n",
    "    \"Guitar\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2166aa2c",
   "metadata": {},
   "source": [
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "We chose a large number of topics to ensure that the final dataset would be rich, diverse, and well-populated.\n",
    "\n",
    "Not every Wikipedia article always works — sometimes a page doesn’t exist, fails to load, or doesn’t contain usable sentence structure.  \n",
    "By including a wide mix of political, social, scientific, and controversial topics, we increase the chances that most will work.  \n",
    "This guarantees that even if some articles are skipped or throw errors, we still end up with a **good-sized, well-balanced dataset** for bias analysis.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "**Randy's Update**\n",
    "\n",
    "I added 33 seemingly boring topics to serve as controls, and I dropped a few articles with only 1 sentence. You can see below that they are actually pretty well distributed among the bias scores. We might want to look at a few examples of each at a sentence level during the presentation to talk about why, for example, Islamophobia and Caterpillar have such high bias scores while Gun Control and NATO are so low. We could also spend more time choosing interesting examples, but I think this is a decent start. \n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110657a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentence_dataset = process_wikipedia_articles(topics, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b06f649",
   "metadata": {},
   "source": [
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "This line runs the full bias detection process on all the topics in the `topics` list using the trained model `pipeline`.\n",
    "\n",
    "It goes through each article, predicts bias sentence-by-sentence, and saves the results into a single CSV file.  \n",
    "By the end, you'll have one dataset with bias predictions across all the selected Wikipedia topics, ready for further analysis or visualization.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317270ae-318d-4b3f-a9ab-da69f7e71702",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentence_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c37273-7d29-471d-a50b-6a2761bca87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_articles(titles, model):\n",
    "    summaries = []\n",
    "\n",
    "    for title in titles:\n",
    "        try:\n",
    "            result = predict_bias_from_article(title, model)\n",
    "            summaries.append(\n",
    "                {\n",
    "                    \"title\": title,\n",
    "                    \"bias_score\": result[\"bias_score\"],\n",
    "                    \"biased_sentences\": result[\"biased_sentences\"],\n",
    "                    \"total_sentences\": result[\"total_sentences\"],\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"error: {title} — {e}\")\n",
    "\n",
    "    summaries_df = pd.DataFrame(summaries)\n",
    "    summaries_df[\"percent_biased\"] = (\n",
    "        summaries_df[\"biased_sentences\"] / summaries_df[\"total_sentences\"]\n",
    "    ).round(3)\n",
    "\n",
    "    return summaries_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58511125-eea6-4aaf-84fa-da160c9b9014",
   "metadata": {},
   "source": [
    "────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "**Randy's Update**\n",
    "\n",
    "This function takes in a list of Wikipedia article titles and runs **bias prediction** on every article.\n",
    "\n",
    "For each title:\n",
    "- It fetches the article and produces a bias score by aggregating sentence-level bias predictions.\n",
    "- It returns a dataset with article titles, bias scores, the number of biased sentences, and the number of total sentences, and the percent of sentences biased.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4fa2c-6d9d-444a-ab7c-59418458e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_article_dataset = summarize_articles(topics, pipeline).sort_values(\n",
    "    by=\"bias_score\", ascending=False\n",
    ")\n",
    "\n",
    "wiki_article_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9194ff-e338-468e-bd46-f2bb10035b0a",
   "metadata": {},
   "source": [
    "# Exploring the Sentence and Article Datasets\n",
    "\n",
    "### How long are the texts in the sentence dateset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2942e3c4-5652-4c59-a28c-700b455d7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentence_wordcount = (\n",
    "    wiki_sentence_dataset[\"combined_text\"].str.lower().str.split().str.len()\n",
    ")\n",
    "wiki_sentence_wordcount.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ab069-9ac2-4e38-aed1-ffc0a6f44d78",
   "metadata": {},
   "source": [
    "The longest sentence in the dataset contains 463 words, while the shortest contains just 1 word. The median word count is 21, and the standard deviation is 14.32. That includes some very long sentences, but on the whole, these texts are similar in length to the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb94b6-8443-46fe-8616-60f935a2db07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean = wiki_sentence_wordcount.mean()\n",
    "std = wiki_sentence_wordcount.std()\n",
    "wiki_sentence_wordcount_filtered = wiki_sentence_wordcount[\n",
    "    wiki_sentence_wordcount <= mean + 3 * std\n",
    "]\n",
    "outliers = len(wiki_sentence_wordcount) - len(wiki_sentence_wordcount_filtered)\n",
    "\n",
    "print(\n",
    "    f\"Word Count Outliers : {outliers} ({100*outliers/len(wiki_sentence_wordcount):0.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87107b5e-36ea-4dbf-89a7-8ee42e350ff5",
   "metadata": {},
   "source": [
    "There are 258 sentences greater than 3 standard deviations above the mean in length, representing less than 1% of the sentence dataset. \n",
    "\n",
    "To visualize this data (after removing outliers to make the data legible):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12522479-cfc5-451a-9941-7a9889f15064",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentence_wordcount_filtered.plot(\n",
    "    title=\"Word Counts in the Wiki Sentence Dataset\",\n",
    "    kind=\"hist\",\n",
    "    bins=30,\n",
    "    figsize=[8, 5],\n",
    "    xlabel=\"Words in text\",\n",
    "    ylabel=\"Number of entries\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81065bcd-488b-46a5-8e19-9cc766e280c6",
   "metadata": {},
   "source": [
    "We can see something loosely resembling a right-skewed normal distribution here, ranging from 1 word to more than 60. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cbc552-1b2c-4161-89be-574fdf53de5b",
   "metadata": {},
   "source": [
    "### How many words match the bias word lexicon in the sentence dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf790b16-f4bf-43bd-8c8c-7cae50e21ffb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_sentence_dataset[\"lexicon_match_count\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba82fadf-ee86-441c-94e0-bfab1a1b6c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentence_dataset[\"lexicon_match_count\"].plot(\n",
    "    title=\"Lexicon Match Count in SG2\",\n",
    "    kind=\"hist\",\n",
    "    bins=30,\n",
    "    figsize=[8, 5],\n",
    "    xlabel=\"Lexicon Match Count\",\n",
    "    ylabel=\"Number of Sentences\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0474ce77-3439-41e7-806c-114ce4a8040c",
   "metadata": {},
   "source": [
    "The vast majority of sentences in the sentence dataset (95%) have no matching words from the bias word lexicon included with the BABE source, even more than in the SG2 dataset. A small number of sentences have 1 or 2 matching words (5%), and a negligible number (46 sentences) have between 3 and 7 matching words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62d92db-8671-4753-99e3-6fa2129db8ee",
   "metadata": {},
   "source": [
    "### How biased are the sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d849d-85c9-498f-bc09-2bfccf6accfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_sentence_dataset[\"bias_prediction\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae9d48e-028a-4b2b-9a59-0dbb9501e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentence_dataset[\"bias_probability\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af05dcd-144e-476f-adfb-e39ae08cd386",
   "metadata": {},
   "source": [
    "About 1/3 (33%) of the sentences are deemed biased by the model. Bias scores range between 0 and 1, with a mean of 0.39 and a standard deviation of 0.22. Most sentences have moderate bias scores between 0.23 and 0.56. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6131241-33dc-4d2f-afc2-e09c98fe5f2e",
   "metadata": {},
   "source": [
    "### How long are the texts in the article dateset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b87328-f3bb-429d-9d90-a909ebe2e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_article_dataset[\"total_sentences\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec521c3-5194-42ee-824f-2881375802e8",
   "metadata": {},
   "source": [
    "The longest article in the dataset contains 753 sentences, while the shortest contains 49 sentences. The median sentence count is 310, and the standard deviation is 197."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e9bfe-476d-4351-94e5-d854238e54bb",
   "metadata": {},
   "source": [
    "### How biased are the articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78343c0-c24b-4a74-a4d2-ed7411a6b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(wiki_article_dataset[\"bias_score\"].describe())\n",
    "wiki_article_dataset[\"percent_biased\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee7c82-7835-48b8-897f-bd6f267fa7a6",
   "metadata": {},
   "source": [
    "The bias scores for all 79 articles range from 0.28 to 0.55, with a median of 0.42 and a standard deviation of 0.06. Most of the bias scores are moderate without any particularly extreme values. The percentage of sentences that are biased per article ranges from 19% to 65%, with a median of 40% and a standard deviation of 11%. \n",
    "\n",
    "The top and bottom 25 article titles according to bias score are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60edf50-611e-4bf3-8b0d-0eea18cb4c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(wiki_article_dataset.head(25))\n",
    "wiki_article_dataset.tail(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcb79f4-95c9-48f1-a8d0-0e28de07e487",
   "metadata": {},
   "source": [
    "It is instructive to look at a few examples in more detail. Let's explore the most and least biased articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412e2b3-3424-4cd7-8493-4828375b8ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title = \"Gender identity\"\n",
    "results = predict_bias_from_article(test_title, pipeline)\n",
    "\n",
    "test_score = wiki_article_dataset[wiki_article_dataset[\"title\"] == test_title][\n",
    "    \"bias_score\"\n",
    "].item()\n",
    "test_percent = (\n",
    "    100\n",
    "    * wiki_article_dataset[wiki_article_dataset[\"title\"] == test_title][\n",
    "        \"percent_biased\"\n",
    "    ].item()\n",
    ")\n",
    "print(\n",
    "    f\"Bias Score for the {test_title} article: {test_score} ({int(test_percent)}% of sentences biased)\"\n",
    ")\n",
    "\n",
    "print(\"Top 10 most biased sentences:\")\n",
    "for sent, prob in sorted(\n",
    "    zip(results[\"sentences\"], results[\"probabilities\"]),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True,\n",
    ")[:10]:\n",
    "    print(f\"⚠️ {round(prob, 3)}: {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4910743-7d12-4780-a366-f1937db83046",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title = \"Russian invasion of Ukraine\"\n",
    "results = predict_bias_from_article(test_title, pipeline)\n",
    "\n",
    "test_score = wiki_article_dataset[wiki_article_dataset[\"title\"] == test_title][\n",
    "    \"bias_score\"\n",
    "].item()\n",
    "test_percent = (\n",
    "    100\n",
    "    * wiki_article_dataset[wiki_article_dataset[\"title\"] == test_title][\n",
    "        \"percent_biased\"\n",
    "    ].item()\n",
    ")\n",
    "print(\n",
    "    f\"Bias Score for the {test_title} article: {test_score} ({int(test_percent)}% of sentences biased)\"\n",
    ")\n",
    "\n",
    "print(\"Top 10 most biased sentences:\")\n",
    "for sent, prob in sorted(\n",
    "    zip(results[\"sentences\"], results[\"probabilities\"]),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True,\n",
    ")[:10]:\n",
    "    print(f\"⚠️ {round(prob, 3)}: {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7dd34-dd7c-4360-8a23-b90c82852450",
   "metadata": {},
   "source": [
    "For the most biased article (Gender identity), the top ten most biased sentences all have scores greater than 0.9. They include sensitive words that might contribute to the high bias score, such as \"faulty\", \"circumcision\", \"genitalia\", \"sex\", \"punished\" \"rhetoric\", \"misgendering\", and \"deadnaming.\" \n",
    "\n",
    "The least biased article (Russian invasion of Ukraine) has only one sentence scoring above 0.9, and the top ten most biased sentences have scores as low as 0.79. That explains the lower bias score for the article. These sentences also include potentially sensitive words that might be flagged as biased, such as \"missile\", \"evading\", \"sanction\", \"destroyed\", \"cancelled\", \"nuclear\", \"fearlessly\", and \"looting\". \n",
    "\n",
    "The number and nature of sensitive words and phrases reflecting subtle linguistic bias determine the bias scores of the respective articles. Examining a few examples shows that all articles have some such linguistic features, but some have more than others. The training data from BABE is specifically devoted to news articles in the media, so our model concentrates on language considered biased in that context. Since Wikipedia articles are somewhat different in tone and style, what is considered biased in the context of news might not always align with more encyclopedic language. For example, discussion of sex and genitalia might be considered inappropriate in media coverage, despite being presented factually in an encyclopedic context. Similarly, unbiased news articles about war might make the model less sensitive to words about violence and weaponry. \n",
    "\n",
    "Overall, these bias scores are a reasonable place to start for evaluating bias in Wikipedia articles, even if there might be some misalignment between news stories and encyclopedia entries. \n",
    "\n",
    "Future work on this topic should consider several enhancements:\n",
    "- Use a dataset specifically devoted to Wikipedia articles, such as the WikiBias Multi-Span Bias Corpus or the Wikipedia Neutrality Corpus, to ensure more directly relevant training examples.\n",
    "- Leverage more advanced word embeddings, such as BERT or RoBERTa, to capture richer contextual and semantic information.\n",
    "- Explore deeper neural architectures, including multi-layer models built on top of pre-trained transformers, to better model linguistic nuance.\n",
    "\n",
    "These strategies would provide more targeted data, harness more sophisticated language representations, and employ more powerful models — all of which could improve performance in identifying subtle bias in Wikipedia articles. Given the time constraints of this project, we opted for a simpler, more accessible approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20280cd-0cfd-41ee-89c3-51b277f160db",
   "metadata": {},
   "source": [
    "**Ken's Update**\n",
    "\n",
    "In addition to some exploratory analysis of our main data, I wanted to compare the way using different training data might affect the results. `SG1` and `MBIC` contain texts that are a subset of `SG2`'s texts, but they used different techniques for labeling bias: `SG1` was labeled by a panel of experts, where `MBIC`'s labels were crowdsourced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15694a48",
   "metadata": {},
   "source": [
    "# Comparing SG1 to MBIC\n",
    "\n",
    "As a bonus objective of sorts, let's compare SG1 to MBIC. Both of these data sets contain 1700 texts, which are a subset of the texts in SG2. However, the key difference is that they were rated for bias by different groups: SG1 was rated by a panel of experts, while MBIC used crowdsourcing.\n",
    "\n",
    "If we train two models identically, with the sole difference being whether SG1 or MBIC's labels were used, **which one finds more bias in the Wikipedia test set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd45317",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg1 = pd.read_csv(os.path.join(dataset_path, \"final_labels_SG1.csv\"), sep=\";\")\n",
    "df_mbic = pd.read_csv(os.path.join(dataset_path, \"final_labels_MBIC.csv\"), sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5315136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing two typographical errors present in MBIC's data--discovered these while working in another notebook\n",
    "\n",
    "df_mbic.loc[1544, \"text\"] = df_sg1.loc[423, \"text\"]\n",
    "df_mbic.loc[923, \"text\"] = df_sg1.loc[453, \"text\"]\n",
    "df_mbic[\"text\"].isin(df_sg1[\"text\"]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2e33c1",
   "metadata": {},
   "source": [
    "While I discovered SG1 is also missing some `news_link`s that are present in MBIC, I'll be dropping that data anyway since it's not going to be relevant to the test data, so there's little point in fixing it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e667e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentence_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentence_dataset.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b1606a",
   "metadata": {},
   "source": [
    "The `wiki_sentence_dataset` from earlier has a `combined_text`, which is equivalent to `text` from the SG1 and MBIC sets. Let's take a closer look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524cd745",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentence_dataset[\"combined_text\"].str.lower().str.split().str.len().plot(\n",
    "    title=\"Word counts in Wikipedia test set\",\n",
    "    kind=\"hist\",\n",
    "    bins=100,\n",
    "    figsize=[8, 5],\n",
    "    xlabel=\"Words in text\",\n",
    "    ylabel=\"Number of articles\",\n",
    "    xlim=[0, 100],\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90894c87",
   "metadata": {},
   "source": [
    "A few interesting things: the test set contained a small number of 100+ outliers, necessitating using `xlim` for the chart. It's also just a far bigger test set than the training sets, reflecting over 27000 sentences pulled from scores of articles. \n",
    "\n",
    "Moreover:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentence_dataset[\n",
    "    wiki_sentence_dataset[\"combined_text\"].str.lower().str.split().str.len() <= 5\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc503779",
   "metadata": {},
   "source": [
    "The Wikipedia data contains a lot of short sentences reflecting things like sources, sentences cut off midway through due to abbreviations, and some actual coherent sentences as well, such as \"Trump pleaded not guilty.\"\n",
    "\n",
    "SG1, MBIC, and SG2 contain extremely few short thoughts like these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e23a74",
   "metadata": {},
   "source": [
    "# Exploring SG1 and MBIC\n",
    "\n",
    "### Did SG1 or MBIC find more biased sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mbic contains extra columns not present in df_sg1; we'll drop those to make the comparison more 1:1\n",
    "droppables = [\"group_id\", \"num_sent\", \"article\"]\n",
    "df_mbic = df_mbic.drop(droppables, axis=1)\n",
    "df_mbic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_biases = pd.concat(\n",
    "    [df_mbic[\"label_bias\"].value_counts(), df_sg1[\"label_bias\"].value_counts()], axis=1\n",
    ")\n",
    "label_biases.columns = [\"MBIC\", \"SG1\"]\n",
    "label_biases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf11db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_biases.plot(\n",
    "    title=\"Label Bias according to MBIC and SG1 (1700 sentences)\",\n",
    "    kind=\"bar\",\n",
    "    rot=30,\n",
    "    xlabel=\"Bias Category\",\n",
    "    ylabel=\"Sentences Found\",\n",
    "    figsize=[10, 4],\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e64592-e090-4127-ba22-8b9867f51912",
   "metadata": {},
   "source": [
    "MBIC, which crowdsourced labeling, was more sensitive to bias than SG1. \n",
    "\n",
    "### How many texts did SG1 and MBIC disagree on?\n",
    "\n",
    "We can concatenate the `label_bias` from one training set to the other. We will remove it later for training, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991df513",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mbic[\"sg1_label_bias\"] = df_sg1[\"label_bias\"]\n",
    "label_mismatches = df_mbic.loc[df_mbic[\"label_bias\"] != df_mbic[\"sg1_label_bias\"]]\n",
    "label_mismatches.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23feb000",
   "metadata": {},
   "source": [
    "1001 of the 1700 labels caused disagreement! We can also find the ones that do match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_matches = df_mbic.loc[df_mbic[\"label_bias\"] == df_mbic[\"sg1_label_bias\"]]\n",
    "label_matches.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880492d9-6486-4d80-9bf9-71b5d762833b",
   "metadata": {},
   "source": [
    "We can then compare these two subsets of data. For example:\n",
    "\n",
    "### Were some news outlets more likely to cause diagreement between SG1 and MBIC?\n",
    "\n",
    "We can calculate this by dividing the `value_counts` of `outlet` in the mismatch dataframe to either MBIC or SG1's equivalent (the two datasets are identical on that column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreements = label_mismatches[\"outlet\"].value_counts() / (\n",
    "    df_sg1[\"outlet\"].value_counts()\n",
    ")\n",
    "disagreements.plot(\n",
    "    title=\"Proportion of bias disagreement between MBIC and SG1 by outlet\",\n",
    "    kind=\"bar\",\n",
    "    xlabel=\"Outlet\",\n",
    "    rot=30,\n",
    "    ylabel=\"Percentage mismatch\",\n",
    "    figsize=[10, 4],\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "disagreements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8362e5",
   "metadata": {},
   "source": [
    "We can see that Fox News was somewhat more likely to cause disagreement between SG1 and MBIC's rating systems, but all of the rest were within six percentage points of each other.\n",
    "\n",
    "Fox is relatively underrepresented in this data, so it could just be explained by random variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80962701",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg1[\"outlet\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42410e9b",
   "metadata": {},
   "source": [
    "## Do SG1 and MBIC differ in what kinds of opinions are biased?\n",
    "\n",
    "We can compare the `type` column, which indicates the leaning of the outlet the text came from, and then also look at the `label_bias` column to see whether the entry was considered biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c0ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg1_leanings = df_sg1.groupby([\"type\", \"label_bias\"]).count()\n",
    "sg1_leanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbic_leanings = df_mbic.groupby([\"type\", \"label_bias\"]).count()\n",
    "mbic_leanings.loc[\"center\"].loc[\"Biased\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3406ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "leanings = pd.concat([sg1_leanings[\"outlet\"], mbic_leanings[\"outlet\"]], axis=1)\n",
    "leanings.columns = [\"SG1\", \"MBIC\"]\n",
    "leanings = leanings.loc[[\"left\", \"center\", \"right\"]]\n",
    "leanings.plot(\n",
    "    title=\"Leanings (SG1)\",\n",
    "    kind=\"bar\",\n",
    "    rot=30,\n",
    "    figsize=[12, 4],\n",
    "    xlabel=\"Type and Bias Verdict\",\n",
    "    ylabel=\"Entries\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc5c47e",
   "metadata": {},
   "source": [
    "This chart shows a few interesting traits:\n",
    "\n",
    "- MBIC consistently found more biased results than SG1 did and fewer non-biased results, across all three ideological categories\n",
    "- The difference is closer for right-leaning sources\n",
    "    - MBIC found slightly more left-leaning biased texts than right-leaning ones; SG1 found slightly more right-leaning biased texts than left-leaning ones\n",
    "- Both sets of labelers generally thought center-leaning sources were usually not biased, while the other two ideological categories were usually biased\n",
    "- Among \"No agreement\" texts (where the labelers within each group couldn't decide whether a text was biased), SG1 and MBIC both had disagreements only occasionally\n",
    "    - This means that the raters from both groups usually thought it was pretty clear whether a text was biased or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d276a50",
   "metadata": {},
   "source": [
    "# Comparing SG1 and MBIC as training data\n",
    "\n",
    "We'll use two logistic regressions, one trained on MBIC's data and one on SG1's data, with their respective `label_bias` columns as targets. We'll then test the models against some scraped Wikipedia pages to see how similar or different the results are. Given the above, **my hypothesis is that MBIC will find more bias in Wikipedia than SG1 will**, but we'll see!\n",
    "\n",
    "In order to get the models ready for training, we have to do some preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44502f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the extra column we added earlier\n",
    "df_mbic = df_mbic.drop(\"sg1_label_bias\", axis=1)\n",
    "df_mbic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from what we did to SG2 earlier; removing 'No agreement' texts, and encoding bias labels\n",
    "df_sg1 = df_sg1[df_sg1[\"label_bias\"] != \"No agreement\"]\n",
    "df_sg1[\"label\"] = df_sg1[\"label_bias\"].map({\"Biased\": 1, \"Non-biased\": 0})\n",
    "df_sg1.drop(columns=\"label_bias\", inplace=True)\n",
    "\n",
    "df_mbic = df_mbic[df_mbic[\"label_bias\"] != \"No agreement\"]\n",
    "df_mbic[\"label\"] = df_mbic[\"label_bias\"].map({\"Biased\": 1, \"Non-biased\": 0})\n",
    "df_mbic.drop(columns=\"label_bias\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b314aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating 'lexicon_match_count\n",
    "\n",
    "df_sg1[\"lexicon_match_count\"] = df_sg1[\"text\"].apply(\n",
    "    lambda x: sum(word in bias_words_set for word in str(x).lower().split())\n",
    ")\n",
    "\n",
    "df_mbic[\"lexicon_match_count\"] = df_mbic[\"text\"].apply(\n",
    "    lambda x: sum(word in bias_words_set for word in str(x).lower().split())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b9f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding headlines\n",
    "\n",
    "df_sg1 = df_sg1.merge(\n",
    "    headline_full[[\"url\", \"title\"]], left_on=\"news_link\", right_on=\"url\", how=\"left\"\n",
    ")\n",
    "\n",
    "df_sg1[\"combined_text\"] = df_sg1.apply(\n",
    "    lambda row: (\n",
    "        f\"{row['title']}. {row['text']}\" if pd.notnull(row[\"title\"]) else row[\"text\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# df_sg1.drop(columns=['url', 'title'], inplace=True)\n",
    "\n",
    "df_mbic = df_mbic.merge(\n",
    "    headline_full[[\"url\", \"title\"]], left_on=\"news_link\", right_on=\"url\", how=\"left\"\n",
    ")\n",
    "\n",
    "df_mbic[\"combined_text\"] = df_mbic.apply(\n",
    "    lambda row: (\n",
    "        f\"{row['title']}. {row['text']}\" if pd.notnull(row[\"title\"]) else row[\"text\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b975bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding 'combined_text'\n",
    "\n",
    "df_sg1[\"combined_text\"] = df_sg1.apply(\n",
    "    lambda row: (\n",
    "        row[\"combined_text\"]\n",
    "        if row[\"combined_text\"] != row[\"text\"]\n",
    "        else f\"[NO_TITLE] {row['text']}\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "df_mbic[\"combined_text\"] = df_mbic.apply(\n",
    "    lambda row: (\n",
    "        row[\"combined_text\"]\n",
    "        if row[\"combined_text\"] != row[\"text\"]\n",
    "        else f\"[NO_TITLE] {row['text']}\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd622c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05cadeb",
   "metadata": {},
   "source": [
    "To match what we did with SG2, we'll use `combined_text` as our feature and `label` as our target. SG2 hit a ROC-AUC of 0.804 with these parameters, for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicating SG2 pipeline for SG1\n",
    "\n",
    "classifier_SG1 = LogisticRegression(\n",
    "    max_iter=1000, penalty=\"l2\", C=1.0, solver=\"liblinear\"\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text\", vectorizer, text_feature),\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_sg1 = Pipeline(\n",
    "    steps=[(\"preprocessing\", preprocessor), (\"classifier\", classifier_SG1)]\n",
    ")\n",
    "\n",
    "X = df_sg1[[text_feature] + numeric_features]\n",
    "y = df_sg1[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "pipeline_sg1.fit(X_train, y_train)\n",
    "y_pred = pipeline_sg1.predict(X_test)\n",
    "y_proba = pipeline_sg1.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"SG1 ROC-AUC:\", roc_auc_score(y_test, y_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a187ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicating SG2 pipeline for MBIC\n",
    "\n",
    "classifier_MBIC = LogisticRegression(\n",
    "    max_iter=1000, penalty=\"l2\", C=1.0, solver=\"liblinear\"\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text\", vectorizer, text_feature),\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_mbic = Pipeline(\n",
    "    steps=[(\"preprocessing\", preprocessor), (\"classifier\", classifier_MBIC)]\n",
    ")\n",
    "\n",
    "X = df_mbic[[text_feature] + numeric_features]\n",
    "y = df_mbic[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "pipeline_mbic.fit(X_train, y_train)\n",
    "y_pred = pipeline_mbic.predict(X_test)\n",
    "y_proba = pipeline_mbic.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"MBIC ROC-AUC:\", roc_auc_score(y_test, y_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f783fe",
   "metadata": {},
   "source": [
    "With this validation technique, SG2 achieved ROC-AUC of about 0.76, while MBIC fared slightly worse at 0.73. Neither were able to match SG2, but as there's less training data, that's not too surprising. This could also indicate SG2 ratings were less noisy than SG1 or MBIC.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f12460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write CSV file with results\n",
    "sg1_wiki_sentence_dataset = process_wikipedia_articles(\n",
    "    topics, pipeline_sg1, output_file=\"../scraped_data/sg1_wiki_bias_predictions.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbic_wiki_sentence_dataset = process_wikipedia_articles(\n",
    "    topics, pipeline_mbic, output_file=\"../scraped_data/mbic_wiki_bias_predictions.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58344a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg1_wiki_sentence_dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5fb92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbic_wiki_sentence_dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a84ea",
   "metadata": {},
   "source": [
    "We now have two sets of test results, `sg1_wiki_sentence_dataset` and `mbic_wiki_sentence_dataset`, which reflect what happened when we asked our SG1 and MBIC models to predict bias on the Wikipedia articles. `bias_prediction` is a binary yes-no prediction of whether a sentence was biased, while `bias_probability` reflects the certainty that the model thought the sentence might be biased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5401b6d8",
   "metadata": {},
   "source": [
    "### How sensitive was each model to bias?\n",
    "\n",
    "The models both assign `bias_probability` in a range that, in theory, runs from 0-1. In practice, they'll find a more narrow range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea169fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg1_wiki_sentence_dataset[\"bias_probability\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f09c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbic_wiki_sentence_dataset[\"bias_probability\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f468c2",
   "metadata": {},
   "source": [
    "SG1's minimum bias probability was about 13% to MBIC's 20%, the median was 43% to 59%, and the maximums were 98.7% to 99.98%, meaning both models made heavy use of the top range. Notably the 75th percentile sentence for SG1 was only 46%, which is less than the 25th percentile for MBIC, which means MBIC made much more use of the top end of the scale than SG1 did. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987238d8",
   "metadata": {},
   "source": [
    "### How many biased sentences did each model find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9324a95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg1_wiki_sentence_dataset.groupby(\"bias_prediction\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964b041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbic_wiki_sentence_dataset.groupby(\"bias_prediction\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ee3a08",
   "metadata": {},
   "source": [
    "The difference is stark: of our 27222 sentences, SG1 thought merely 2960 were biased, while MBIC detected bias in 26182 of them! That's 11% and 96% respectively. (By comparison, the full SG2 model found about 26% bias.) However, there's a catch: as we discovered during EDA, a fair few of the test set sentences are extremely short:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddfe414",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg1_wiki_sentence_dataset[\n",
    "    \"combined_text\"\n",
    "].str.lower().str.split().str.len().value_counts().sort_index().head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e95681",
   "metadata": {},
   "source": [
    "Our data contains 1172 texts of four words or fewer. Many of these are not coherent thoughts, including things like references and sentences that are cut midway by abbreviations. Does text length affect our models' perceptions here? For a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc63d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg1_wiki_sentence_dataset[\n",
    "    sg1_wiki_sentence_dataset[\"combined_text\"].str.lower().str.split().str.len() < 5\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70219f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg1_wiki_sentence_dataset[\n",
    "    sg1_wiki_sentence_dataset[\"combined_text\"].str.lower().str.split().str.len() >= 5\n",
    "].groupby(\"bias_prediction\").count() / 27222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf53c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbic_wiki_sentence_dataset[\n",
    "    mbic_wiki_sentence_dataset[\"combined_text\"].str.lower().str.split().str.len() >= 5\n",
    "].groupby(\"bias_prediction\").count() / 27222"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea62adff",
   "metadata": {},
   "source": [
    "SG1's predictions remained about the same, while MBIC found slightly fewer biased sentences--11% and 92% respectively. That's still a huge gap, but considering those short sentences only made up about 4% of the test set, it's surprising they caused that much of a shift for MBIC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b67460",
   "metadata": {},
   "source": [
    "### Which articles are the most biased and least biased?\n",
    "\n",
    "We can group the data sets by article title to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b093bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg1_top10_articles = (\n",
    "    sg1_wiki_sentence_dataset.groupby(\"article_title\")[\"bias_prediction\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "sg1_top10_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedbeee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbic_top10_articles = (\n",
    "    mbic_wiki_sentence_dataset.groupby(\"article_title\")[\"bias_prediction\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "mbic_top10_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a1b8dd",
   "metadata": {},
   "source": [
    "Of the top 10 articles from each model:\n",
    "\n",
    "- SG1 largely found bias in articles about media organizations (New York Times, Fox News, Breitbart News, MSNBC) as well as current US President Donald Trump and several political issues and organizations (Islamophobia, critical race theory, QAnon, etc)\n",
    "- MBIC found four articles made of entirely biased sentences: \"Border wall,\" \"Photosynthesis,\" \"Sand dune,\" and \"Symbiosis.\"\n",
    "- No article appeared on both lists.\n",
    "\n",
    "MBIC's results here are quite odd. We can investigate more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92955b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbic_wiki_sentence_dataset[\n",
    "    mbic_wiki_sentence_dataset[\"article_title\"] == \"Photosynthesis\"\n",
    "].sort_values(by=\"bias_probability\", axis=0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae62c5",
   "metadata": {},
   "source": [
    "Notably, the bias probability by the MBIC model isn't all that high--but it did consistently break the 47% threshold used by SG2.\n",
    "\n",
    "If we instead go by the mean bias probability, rather than the percentage of sentences predicted to be biased, we may find different results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89388035",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg1_top10_articles = (\n",
    "    sg1_wiki_sentence_dataset.groupby(\"article_title\")[\"bias_probability\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "sg1_top10_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a6a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbic_top10_articles = (\n",
    "    mbic_wiki_sentence_dataset.groupby(\"article_title\")[\"bias_probability\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "mbic_top10_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff3bf13",
   "metadata": {},
   "source": [
    "Going by probability, the top 10 lists are a much closer match, with 8 out of 10 articles appearing on both lists. It's likely that this means the threshold for predicting bias needs to be adjusted for MBIC, which is a much more sensitive model.\n",
    "\n",
    "What about the bottom 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a98b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg1_bottom10_articles = (\n",
    "    sg1_wiki_sentence_dataset.groupby(\"article_title\")[\"bias_probability\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=True)\n",
    "    .head(10)\n",
    ")\n",
    "sg1_bottom10_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b385ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbic_bottom10_articles = (\n",
    "    mbic_wiki_sentence_dataset.groupby(\"article_title\")[\"bias_probability\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=True)\n",
    "    .head(10)\n",
    ")\n",
    "mbic_bottom10_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a14edbc",
   "metadata": {},
   "source": [
    "Some insights:\n",
    "\n",
    "- SG1 and MBIC agreed that certain articles of a political nature were relatively unbiased, such as NATO and the Russian invasion of Ukraine. \n",
    "- They also each had a mix of mundane topics, but mostly not the same ones (SG1 thought the baseball article was relatively nonbiased, while MBIC felt that way about train stations, for instance.)\n",
    "- The only mundane topics that appeared on both lists were Nintendo, the video game company; and Milan, the city located in northern Italy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b0474",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We were able to accomplish thorough bias analysis with respect to our selection of Wikipedia articles.\n",
    "\n",
    "- We trained a model on the canonical \"SG2\" data set to detect bias.\n",
    "    - SG2 contains sentences pulled from news articles across several sources and labels them for bias.\n",
    "    - Validation steps resulted in a ROC-AUC score of 0.809.\n",
    "- We scraped Wikipedia for articles, then measured them using the model we trained.\n",
    "    - 79 valid articles were found.\n",
    "- We assigned each Wikipedia article a `bias_score` equal to the number of sentences flagged for bias divided by the total number of sentences.\n",
    "    - Articles ranged from 0.155 to 0.76, meaning most articles contained at least some sentences that might reflect bias.\n",
    "    - This suggests Wikipedia's level of bias varies from article to article.\n",
    "- We also trained two additional models on other data sets, \"SG1\" and \"MBIC,\" for comparison.\n",
    "    - Each data set is identical except that SG1's sentences were labeled by a panel of experts, while MBIC's labels were crowdsourced.\n",
    "    - To keep the comparison as close as possible, SG2 was not used for this section, as it contains additional sentences.\n",
    "    - We did exploratory analysis to find differences in the way the two sets of data labeled bias.\n",
    "        - MBIC was generally more sensitive to bias, especially bias in sources perceived as left-leaning.\n",
    "    - After running the same preprocessing and training that SG2 went through, we determined ROC-AUC scores:\n",
    "        - SG1 scored about 0.75\n",
    "        - MBIC scored about 0.73\n",
    "    - Against the Wikipedia articles, SG2 thought about 11% were biased, while MBIC found bias in over 90% of them!\n",
    "        - This was much greater than the difference in the training data EDA would suggest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
